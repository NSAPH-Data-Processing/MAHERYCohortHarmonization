---
title: "Tracking Input Data"
output: rmarkdown::html_vignette
author: "Tinashe M. Tapera"
vignette: >
  %\VignetteIndexEntry{Tracking Input Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r}
library(MAHERYCohortHarmonization)
```

<!-- WARNING - This vignette is generated by {fusen} from dev/flat_tracking_inputs.Rmd: do not edit by hand -->




# Tracking Inputs

In this notebook, we're developing functions to track all of the input data. Importantly,
this data is stored on Google Drive and must not be shared publicly on the internet,
so we will need to use programmatic credentials to access it.

Let's first create a function that will log us in to google drive with the appropriate credentials.
For this, we'll use the `googledrive` package from `tidyverse`.

We can start the process of authorization using the `drive_auth()` function. This requires
access to the a special JSON file that contains the credentials for the google drive account.

To enable non-interactive access to Google Drive from this R package, we use a _Google Cloud 
service account_ for authentication. This allows scripts and functions to interact with 
Google Drive without requiring manual sign-in or browser-based authentication. It’s particularly
useful for automated workflows, scheduled tasks, or when deploying the package in cloud 
environments (e.g., on a server like `FASRC`). The service account acts like
a dummy user with its own credentials, and it can be granted access to specific files or 
folders in the CSPH drives. Access to the service account can be controlled via sharing
permissions just like any human Google Drive user.

To set this up, the service account was created via the Google Cloud Console under
a project with the Google Drive API enabled. A JSON key was generated and 
downloaded — this file contains the credentials used to authenticate in R. 
Any Google Drive resources the package needs to access must be explicitly shared with 
the service account’s email (in this case, `data-pipeline@harvard-csph-driveauth.iam.gserviceaccount.com`).
Once shared, the pipeline uses the `googledrive` package in R to authenticate via 
`drive_auth(path = "path/to/key.json")`, allowing Drive operations to run seamlessly.

This authentication method ensures that all users of this package (or future maintainers)
can use a consistent, secured credential file for Drive access. To maintain this setup,
keep the JSON key file in a protected location, and do not commit it to version control. 
If the key ever needs to be regenerated (e.g., due to compromise or rotation), the same
process above can be followed to create a new one.

## Authenticating with Google Drive

If you have the authentication file, you can simply point to it with your `drive_auth()` function in
an interactive session. If this is successful, you should be able to access the Google Drive files
from R with `drive_find()`.

```{r}
#| eval: no

path_to_auth <- "path/to/your/auth.json"
drive_auth(path = path_to_auth)
drive_find()
```


In a non-interactive session, make sure to set the environment variable with `.Renviron`.
```{r}
#| echo: no
#| results: hide
#| message: no
#| warning: no

library(googledrive)
path_to_auth <- Sys.getenv("GOOGLE_DRIVE_AUTH_JSON")
drive_auth(path = path_to_auth)
drive_find() # do not print anything in vignettes but will print interactively
```


### authenticate_google_drive

```{r examples-authenticate_google_drive}
authenticate_google_drive()
```



We can add this to the beginning of our `targets` pipeline.

## Ensuring Files Have Not Changed

Ensuring that files haven't changed is important for reproducibility, but is
also arguably quite tricky with Google drive. Anyone with the link and access
can change the file, and this can break the pipeline. We can't really check
the hash of the file, but we can check the last modified date. This is not
perfect, but it's better than nothing. We can also add checks in the pipeline
via assertions and test_that functions such that the pipeline won't be broken
if the file is changed.

<div class="warning">
  <strong>Warning:</strong> <italic>DO NOT</italic> use file ID identifiers in rendered HTML vignettes,
  as they can be used to access the Google Drive if sharing is enabled. This is a security risk. Instead,
  use the file name and `drive_find(pattern = [REGEX])` to reference files.
</div>

### has_drive_file_changed
    
  
  
```{r example-has_drive_file_changed}
#| eval: no

test_file <- "1X9pd4nOjl33zDFfTjw-_eFL7Qb9_g6VfVFDp1PPae94"
has_drive_file_changed(test_file, lubridate::ymd_hms("2025-04-01 00:00:00", tz = "UTC"))
```

  

## Local Data Paths
Here's a utility for creating data paths in any local environment using `here` and
`fs`:

# create_datapaths
    
  
  
```{r example-create_datapaths}
if(FALSE) {
  create_datapaths()
} 
```

  
  

## Northeast Data

Now that we have access to the Google Drive, we can start tracking the input data. We'll start with the Northeast data.

Time order of the data files:
* Year 2018: openSRP_data_2018.xlsx
* Year 2019: Dharma_followup_2020plus.xlsx
* Beyond 2019:  Dharma_2019_2020.csv

First, we simply track the state of these input files from googledrive. We'll simply
use a list to store the file names and their IDs, and then pass that list to 
the target pipeline.

# download_to_local
    
  
  
```{r example-download_to_local}
#| eval: no

authenticate_google_drive()
test_file <- "openSRP_dictionary.xlsx"
fpath <- download_to_local(test_file, here::here())
file.exists(fpath)
file.remove(fpath)
```

  
  

With this in mind, we can likely dedicate a section to each of the files,
using a function to track each one.

